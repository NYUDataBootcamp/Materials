{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Pandas 4:  Combining data  \n",
    "\n",
    "Sometimes we need to combine data from two or more dataframes.  That's colloquially known as a **merge** or a **join**. There are lots of ways to do this.  We do a couple but supply references to more at the end.  \n",
    "\n",
    "Along the way we take an extended detour to review methods for **downloading** and **unzipping** compressed files.  The tools we use here have a broad range of other applications, including web scraping.  \n",
    "\n",
    "Outline:  \n",
    "\n",
    "* [MovieLens data](#movielens).  A collection of movies and individual ratings.  \n",
    "* [Automate file download](#requests).  Use the requests package to get a zipped file, then other tools to unzip it and read in the contents.  \n",
    "* [Merge movie names and ratings](#merge-movies).  Merge information from two dataframes with Pandas' `merge` function.  \n",
    "* [UN population data](#example).  We merge (\"concatenate\") estimates from the past with projections of the future.  \n",
    "\n",
    "**Note: requires internet access to run.**  \n",
    "\n",
    "This IPython notebook was created by Dave Backus, Chase Coleman, Brian LeBlanc, and Spencer Lyon for the NYU Stern course [Data Bootcamp](http://databootcamp.nyuecon.com/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=prelims></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "\n",
    "Import packages, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd             # data package\n",
    "import matplotlib.pyplot as plt # graphics \n",
    "import sys                      # system module, used to get Python version \n",
    "import os                       # operating system tools (check files)\n",
    "import datetime as dt           # date tools, used to note current date  \n",
    "\n",
    "# these are new \n",
    "import requests, io             # internet and input tools  \n",
    "import zipfile as zf            # zip file tools \n",
    "import shutil                   # file management tools \n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "print('\\nPython version: ', sys.version) \n",
    "print('Pandas version: ', pd.__version__)\n",
    "print('Requests version: ', requests.__version__)\n",
    "print(\"Today's date:\", dt.date.today())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=movielens></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MovieLens data \n",
    "\n",
    "The data comes as a zip file that contains several csv's.  We get the details from the README inside.  (It's written in Markdown, so it's easier to read if we use a browser to format it.  Or we could cut and paste into a Markdown cell in an IPython notebook.)  \n",
    "\n",
    "The file descriptions are:  \n",
    "\n",
    "* `ratings.csv`:  each line is an individual film rating with the rater and movie id's and the rating.  Order:  `userId, movieId, rating, timestamp`. \n",
    "* `tags.csv`:  each line is a tag on a specific film.  Order:  `userId, movieId, tag, timestamp`. \n",
    "* `movies.csv`:  each line is a movie name, its id, and its genre.  Order:  `movieId, title, genres`.  Multiple genres are separated by \"pipes\" `|`.   \n",
    "* `links.csv`:  each line contains the movie id and corresponding id's at [IMBd](http://www.imdb.com/) and [TMDb](https://www.themoviedb.org/).  \n",
    "\n",
    "The easy way to input this data is to download the zip file onto our computer, unzip it, and read the individual csv files using `read.csv()`.  But **anyone can do it the easy way**.  We want to automate this, so we can redo it without any manual steps.  This takes some effort, but once we have it down we can apply it to lots of other data sources.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=requests></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate file download \n",
    "\n",
    "We're looking for an automated way, so that if we do this again, possibly with updated data, the whole process is in our code.  Automated data entry involves these steps: \n",
    "\n",
    "* Get the file.  We use the [requests](http://docs.python-requests.org/) package, which handles internet files and comes pre-installed with Anaconda. This kind of thing was hidden behind the scenes in the Pandas `read_csv` function, but here we need to do it for ourselves. The package authors add:  \n",
    ">Recreational use of other HTTP libraries may result in dangerous side-effects, including: security vulnerabilities, verbose code, reinventing the wheel, constantly reading documentation, depression, headaches, or even death.\n",
    "* Convert to zip.   Requests simply loads whatever's at the given url. The [io](https://docs.python.org/3.5/library/io.html) module's `io.Bytes` reconstructs it as a file, here a zip file.  \n",
    "* Unzip the file.  We use the [zipfile](https://docs.python.org/3.5/library/zipfile.html) module, which is part of core Python, to extract the files inside.   \n",
    "* Read in the csv's.  Now that we've extracted the csv files, we use `read_csv` as usual.  \n",
    "\n",
    "We found this [Stack Overflow exchange](http://stackoverflow.com/questions/23419322/download-a-zip-file-and-extract-it-in-memory-using-python3) helpful. \n",
    "\n",
    "**Digression.**  This is probably more than you want to know, but it's a reminder of what goes on behind the scenes when we apply `read_csv` to a url.  Here we grab whatever is at the url.  Then we get its contents, convert it to bytes, identify it as a zip file, and read its components using `read_csv`.  It's a lot easier when this happens automatically, but a reminder what's involved if we ever have to look into the details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get \"response\" from url \n",
    "url = 'http://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "r = requests.get(url) \n",
    "\n",
    "# describe response \n",
    "print('Response status code:', r.status_code)\n",
    "print('Response type:', type(r))\n",
    "print('Response .content:', type(r.content)) \n",
    "print('Response headers:\\n', r.headers, sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert bytes to zip file  \n",
    "mlz = zf.ZipFile(io.BytesIO(r.content)) \n",
    "print('Type of zipfile object:', type(mlz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's in the zip file?\n",
    "mlz.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract and read csv's\n",
    "movies  = pd.read_csv(mlz.open(mlz.namelist()[2]))\n",
    "ratings = pd.read_csv(mlz.open(mlz.namelist()[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what do we have? \n",
    "for df in [movies, ratings]:\n",
    "    print('Type:', type(df))\n",
    "    print('Dimensions:', df.shape)\n",
    "    print('Variables:', list(df))\n",
    "    print('First few rows', df.head(3), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Something to do together.  suppose we wanted to save the files on our computer.  How would we do it? Would we prefer individual csv's or a single zip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment via http://stackoverflow.com/a/18043472/804513\n",
    "with open('test.zip', 'wb') as out_file:\n",
    "    shutil.copyfileobj(io.BytesIO(r.content), out_file)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=merge-movies></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging ratings and movie titles \n",
    "\n",
    "The movie ratings in the dataframe `ratings` give us individual opinions about movies, but they don't include the name of the movie.   Why not?  Rather than include the name every time a movie is rated, the MovieLens data associates each rating with a movie code, than stores the names of movies associatd with each movie code in the dataframe `movies`.  We run across this a lot:  some information is in one data table, other information is in another.  \n",
    "\n",
    "Our **want** is therefore to add the movie name to the `ratings` dataframe.  We say we **merge** the two dataferames.  There are lots of ways to merge.  Here we do one as an illustration.  \n",
    "\n",
    "Let's start by reminding ourselves what we have.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging\n",
    "\n",
    "Here's roughly what's involved in what we're doing.  We take the `movieId` variable from `ratings` and look it up in `movies`.  When we find it, we look up the `title` and add it as a column in `ratings`.  The variable `movieId` is common, so we can use it to link the two dataframes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = pd.merge(ratings, movies,   # left and right df's\n",
    "                 how='left',        # add to left \n",
    "                 on='movieId'       # link with this variable/column \n",
    "                ) \n",
    "\n",
    "print('Dimensions of ratings:', ratings.shape)\n",
    "print('Dimensions of movies:', movies.shape)\n",
    "print('Dimensions of new df:', combo.shape)\n",
    "\n",
    "combo.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv file for future use \n",
    "combo.to_csv('mlcombined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Current directory:\\n', os.getcwd(), sep='')\n",
    "print('List of files:', os.listdir(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** Some of these we know how to do, the others we don't.  For the ones we know, what is the answer?  For the others, what (in loose terms) do we need to be able to do to come up with an answer?  \n",
    "\n",
    "* What is the overall average rating?  \n",
    "* What is the overall distribution of ratings?  \n",
    "* What is the average rating of each movie?  \n",
    "* How many ratings does each movie get?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig, ax = plt.subplots()\n",
    "bins = [bin/100 for bin in list(range(25, 575, 50))]\n",
    "print(bins)\n",
    "combo['rating'].plot(kind='hist', ax=ax, bins=bins, color='blue', alpha=0.5)\n",
    "ax.set_xlim(0,5.5)\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_xlabel('Rating')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=population></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Population \"estimates\" and \"projections\" \n",
    "\n",
    "We look (again) at the UN's  [population data](http://esa.un.org/unpd/wpp/Download/Standard/Population/), specifically the age distribution of the population.  The data comes in two sheets:  *estimates* that cover the period 1950-2015 and *projections* that cover 2016-2100.  Our mission is to combine them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data \n",
    "\n",
    "We start, as usual, by loading the data.  This takes a minute or so.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'http://esa.un.org/unpd/wpp/DVD/Files/'\n",
    "url2 = '1_Indicators%20(Standard)/EXCEL_FILES/1_Population/'\n",
    "url3 = 'WPP2017_POP_F07_1_POPULATION_BY_AGE_BOTH_SEXES.XLSX'\n",
    "url = url1 + url2 + url3 \n",
    "\n",
    "cols = [2, 5] + list(range(6,28))\n",
    "est = pd.read_excel(url, sheetname=0, skiprows=16, parse_cols=cols, na_values=['…'])\n",
    "prj = pd.read_excel(url, sheetname=1, skiprows=16, parse_cols=cols, na_values=['…'])\n",
    "\n",
    "print('Dimensions and dtypes of estimates: ', est.shape, '\\n', est.dtypes.head(), sep='')\n",
    "print('\\nDimensions and dtypes of projections: ', prj.shape, '\\n', prj.dtypes.head(), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment.** Note that they have different numbers of columns.  Let's see where that comes from.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(est)[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(prj)[15:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Clean data \n",
    "\n",
    "Pick a useable subset and fix extra column so that we can combine them.  The problem here is that until 1990, the highest age category was `'80+`.  From 1990 on, we have a finer breakdown.  \n",
    "\n",
    "We fix this by reassigning `'80+'` to `'80-84'` and not worrying that some of these people are 85 or older.  Note that `df.fillna(0.0)` replaces missing values with zeros.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanpop(df, countries, years): \n",
    "    \"\"\"\n",
    "    take df as input and select countries and years \n",
    "    \"\"\"\n",
    "    # rename first two columns \n",
    "    names = list(df) \n",
    "    df = df.rename(columns={names[0]: 'Country', names[1]: 'Year'}) \n",
    "\n",
    "    # select countries and years\n",
    "    newdf = df[df['Country'].isin(countries) & df['Year'].isin(years)] \n",
    "    \n",
    "    return newdf\n",
    "\n",
    "countries = ['Japan']\n",
    "past      = [1950, 2000]\n",
    "future    = [2050, 2100]\n",
    "\n",
    "e = cleanpop(est, countries, past) \n",
    "p = cleanpop(prj, countries, future)\n",
    "\n",
    "# make copie sfor later use \n",
    "ealt = e.copy()\n",
    "palt = p.copy()\n",
    "\n",
    "# fix top-coding in estimates\n",
    "e['80-84'] = e['80-84'].fillna(0.0) + e['80+'].fillna(0.0)\n",
    "e = e.drop(['80+'], axis=1)\n",
    "\n",
    "# check dimensions again\n",
    "print('Dimensions of cleaned estimates: ', e.shape)\n",
    "print('Dimensions of cleaned projections: ', p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check to see if we have the same variables \n",
    "list(e) == list(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge estimates and projections \n",
    "\n",
    "If we have two blocks of data, and want just want to put them on top of each other, we use the Pandas' `concatenate` function.  Ditto two blocks next to each other.  \n",
    "\n",
    "But first we need to fix the difference in the columns of the two dataframes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pd.concat([e, p], axis=0).fillna(0.0)\n",
    "pop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise.** What happens if we try to merge the original dataframes, including the one with the extra `80+` column?  Run the code below and comment on what you get.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "popalt = pd.concat([ealt, palt], axis=0)\n",
    "popalt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape data \n",
    "\n",
    "We **want** age categories in the index (the default x axis in a plot) and the years in the columns.  The country we don't care about because there's only one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop = pop.drop('Country', axis=1)\n",
    "popi = pop.set_index('Year')\n",
    "popi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popi.columns.name = 'Age'\n",
    "popt = popi.T\n",
    "popt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = popt.plot(kind='bar', color='blue', alpha=0.5, \n",
    "               subplots=True, \n",
    "               sharey=True, \n",
    "               figsize=(8,12)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Exercise.**  Use `set_index`, `stack`, and `unstack` to shape the dataframe `popi` into `popt`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "The [Pandas docs](http://pandas.pydata.org/pandas-docs/stable/merging.html) are ok, but we prefer the Data Carpentry [guide](http://www.datacarpentry.org/python-ecology-lesson/04-merging-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
